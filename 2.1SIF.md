#### SIF 模型建立

```python
# 2.1 SIF sentence vector 参考链接https://towardsdatascience.com/fse-2b1ffa791cf9
import numpy as np
REAL = np.float32 
from sklearn.decomposition import TruncatedSVD
from sklearn.decomposition import PCA
import jieba
def sif_vstack(sentences:list, model, alpha=1e-3):
    """Compute the SIF embeddings for a list of sentences
    Parameters
    ----------
    sentences : list
        The sentences to compute the embeddings for
    model : `~gensim.models.base_any2vec.BaseAny2VecModel`
        A gensim model that contains the word vectors and the vocabulary
    alpha : float, optional
        Parameter which is used to weigh each individual word based on its probability p(w).
    Returns
    -------
    numpy.ndarray 
        SIF sentence embedding matrix of dim len(sentences) * dimension
    """
    #sentence_words =[list(jieba.cut(sentence,cut_all=False)) for item in sentences]
    sentence_words = [list(jieba.cut(item,cut_all=False)) for item in sentences]

    output = []
    for s in sentence_words:
        print(s)
        count = 0
        v = np.zeros(size, dtype=REAL) # Summary vector
        for w in s:
            if w in vlookup:
                v += ( alpha / (alpha + (vlookup[w].count / Z))) * vectors[w]
                count += 1
        if count > 0:
            v *= 1/count
        output.append(v)
    results=np.vstack(output).astype(REAL)
    tsvd = TruncatedSVD(n_components=1, n_iter=7, random_state=0)
    tsvd.fit(results)
    u = tsvd.components_[0]
    u = np.multiply(u, u.T)
    for vs in results:
        vs -= np.multiply(u, vs)
    return results
    ```
