##### 1 利用前面两次处理好的分词数据，利用gensim来训练word2vec向量。


```python
#定义读取文件夹的ReadTxtFiles类，有一个文件夹的参数。
import os
import datetime
from gensim.utils import simple_preprocess
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
from gensim import corpora
import threading
class ReadTxtVecFiles(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            myname= threading.currentThread().getName()
            print("process file:",myname+"-"+fname)
            for line in open(os.path.join(self.dirname, fname), encoding='utf-8'):
                #print(line)
                yield line.split()
print("start time:",str(datetime.datetime.now()))
path_to_text_directory = "F:/ai/项目/wikiextractor-master/wikiextractor-master/extracted/AA/parsed/"
outp1="myword2vec.model"
#ReadTxtVecFiles(path_to_text_directory)
#https://rare-technologies.com/word2vec-tutorial/ 
#会循环多次遍历
model = Word2Vec(ReadTxtVecFiles(path_to_text_directory),size=100, window=5, min_count=1, workers=2)
model.save(outp1)
print("end time:",str(datetime.datetime.now()))
```
这里有一个要说明的地方，会发现文件会被反复读取，本来以为程序有问题。后来看到 (https://rare-technologies.com/word2vec-tutorial/) 有说到:
>>>Note to advanced users: calling Word2Vec(sentences, iter=1) will run two passes over the sentences iterator (or, in general iter+1 passes; default iter=5). The first pass collects words and their frequencies to build an internal dictionary tree structure. The second and subsequent passes train the neural model. These two (or, iter+1) passes can also be initiated manually, in case your input stream is non-repeatable (you can only afford one pass), and you’re able to initialize the vocabulary some other way:
```python
start time: 2019-11-16 15:45:40.962014
process file: MainThread-chinesecorpus.txtparse
process file: MainThread-wiki_00s_utf8parse
process file: MainThread-wiki_01s_utf8parse
process file: MainThread-wiki_02s_utf8parse
process file: MainThread-wiki_03s_utf8parse
process file: MainThread-wiki_04s_utf8parse
process file: Thread-133-chinesecorpus.txtparse
process file: Thread-133-wiki_00s_utf8parse
process file: Thread-133-wiki_01s_utf8parse
process file: Thread-133-wiki_02s_utf8parse
process file: Thread-133-wiki_03s_utf8parse
process file: Thread-133-wiki_04s_utf8parse
process file: Thread-136-chinesecorpus.txtparse
process file: Thread-136-wiki_00s_utf8parse
process file: Thread-136-wiki_01s_utf8parse
process file: Thread-136-wiki_02s_utf8parse
process file: Thread-136-wiki_03s_utf8parse
process file: Thread-136-wiki_04s_utf8parse
process file: Thread-139-chinesecorpus.txtparse
process file: Thread-139-wiki_00s_utf8parse
process file: Thread-139-wiki_01s_utf8parse
process file: Thread-139-wiki_02s_utf8parse
process file: Thread-139-wiki_03s_utf8parse
process file: Thread-139-wiki_04s_utf8parse
process file: Thread-142-chinesecorpus.txtparse
process file: Thread-142-wiki_00s_utf8parse
process file: Thread-142-wiki_01s_utf8parse
process file: Thread-142-wiki_02s_utf8parse
process file: Thread-142-wiki_03s_utf8parse
process file: Thread-142-wiki_04s_utf8parse
process file: Thread-145-chinesecorpus.txtparse
process file: Thread-145-wiki_00s_utf8parse
process file: Thread-145-wiki_01s_utf8parse
process file: Thread-145-wiki_02s_utf8parse
process file: Thread-145-wiki_03s_utf8parse
process file: Thread-145-wiki_04s_utf8parse
end time: 2019-11-16 16:37:31.444924
```
