#### 1 文本预处理

使用的汉语新闻csv corpus 包含许多列，实际只需要content列。
使用pandas 主要是方便取某一列。但是pandas读取的时候，容易有编码的问题。 
 所以先把csv 文件转换成utf-8格式，可以采用iconv或者文本打开另存为的时候选择编码格式。
 
```python
#只获取一列的数据。
import pandas as pd
df = pd.read_csv("./export_sql_1558435/sqlResult_1558435utf8.csv",encoding = 'utf-8',usecols = ['content'])
df.to_csv('chinesecorpus.txt',index=False)
```    
#### 2 对文本进行处理。主要使用python 来去除文件中不必要字符
```python
## create dictionarly from file
def get_stopwords():
    #加载停用词表
    stopword_set = set()
    with open("./stopwords/中文停用词表.txt",'r',encoding="utf-8") as stopwords:
        for stopword in stopwords:
            stopword_set.add(stopword.strip("\n"))
    return stopword_set

'''
使用正则表达式解析文本
'''
import jieba
def parse_zhwiki(read_file_path,save_file_path):
    #过滤掉<doc> 和</doc>
    #regex_str = r"(^<doc.*>$|^</doc>$)"
    #过滤掉（空格和）标点符号等
    regex_str1=r"[（ ）▌◆★.@()\-\|!'&?$【】\"\”\“\■\◎\●\/\…\　 　 \\n\:\·]"
    #只读取中文和英文字符。
    #regex_str2=r"[]
    #docmatch=re.compile(regex_str)
    with open(read_file_path,"r",encoding="utf-8") as file:
        #写文件
        with open(save_file_path,"w+",encoding="utf-8") as output:
            content_line = file.readline()
            #获取停用词表
            stopwords = get_stopwords()
      
            while content_line:
                #match_obj = docmatch.match(content_line)
                content_line = content_line.strip("\n")
                if len(content_line) > 0:
                    #if not match_obj:
                    #定义一个字符串变量，表示一篇文章的分词结果
                        article_contents = ""
                        #替换
                        content_line=re.sub(regex_str1,"",content_line)
                        #使用jieba进行分词
                        words = jieba.cut(content_line,cut_all=False)
                        for word in words:
                            if word not in stopwords:
                                article_contents += word+" "
                        output.write(article_contents+'\n')
                content_line = file.readline()

##批量汉语新闻文本分词
import os
def iter_documents(top_directory):
    """
    Generator: iterate over all relevant documents, yielding one
    document (=list of utf8 tokens) at a time.
    """
    # find all .txt documents, no matter how deep under top_directory
    for root, dirs, files in os.walk(top_directory):
        #for fname in filter(lambda fname: fname.endswith('.txt'), files):
            # read each document as one big string
            for file in files:
                file_input_path=os.path.join(top_directory,file)
                file_output_path=os.path.join(top_directory,file+"parse")
                parse_zhwiki(file_input_path,file_output_path)
                print("finished:",file_input_path)
            #document = open(os.path.join(root, fname)).read()
            # break document into utf8 tokens
            #yield gensim.utils.tokenize(document, lower=True, errors='ignore')
iter_documents("F:/ai/项目/wikiextractor-master/wikiextractor-master/extracted/AB")
```
