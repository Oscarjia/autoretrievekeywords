#### 维基百科中文预料处理步骤,参考博文(https://blog.csdn.net/sinat_29957455/article/details/81432846,https://blog.csdn.net/qq_32166627/article/details/68942216)
版权声明：本文为CSDN博主「name_s_Jimmy」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_32166627/article/details/68942216

##### 1 下载原始数据

数据下载地址：https://dumps.wikimedia.org/zhwiki/latest/zhwiki-XXX-pages-articles.xml.bz2
下载的文件是一个大小为1.3G的压缩包，解压后是个5.8G左右的xml文件，内容是网页标签形式的。我们需要抽取出其中的有效信息。

##### 2 使用Wikipedia Extractor抽取正文

Wikipedia Extractor是意大利人用Python写的一个维基百科抽取器，使用非常方便。下载之后直接使用这条命令即可完成抽取，运行时间很快。执行以下命令。
``` linux
$ sudo apt-get install unzip python python-dev python-pip
$ git clone https://github.com/attardi/wikiextractor.git wikiextractor
$ cd wikiextractor
$ sudo python setup.py install
$ ./WikiExtractor.py -b 1024M -o extracted zhwiki-latest-pages-articles.xml.bz2

INFO: Finished 3-process extraction of 1067922 articles in 2122.1s (503.2 art/s)
INFO: total of page: 2416224, total of articl page: 1067922; total of used artic
l page: 1067922
```
参数-b 1024M表示以1024M为单位切分文件，默认是1M。由于最后生成的正文文本约1400M，把参数设置的大一些可以保证最后的抽取结果全部存在一个文件里。这里我们设为1024M，可以分成一个1G的大文件和一个267M的小文件，后续的步骤可以先在小文件上实验，再应用到大文件上。

这里，我们得到了2个文本文件：wiki_00, wiki_01。大小分别为：1024M, 267M。
##### 3 繁体字转为简体字
windows 下面安装opencc有些问题，网上找到了一个替代库。

```linux
pip install opencc-python-reimplemented
python -m opencc -c s2t -i my_simplified_input_file.txt -o my_traditional_output_file.txt
-i <file>, --input <file> Read original text from <file>. (default: None = STDIN)
-o <file>, --output <file> Write converted text to <file>. (default: None = STDOUT)
-c <conversion>
Conversions 轉換
hk2s: Traditional Chinese (Hong Kong standard) to Simplified Chinese
s2hk: Simplified Chinese to Traditional Chinese (Hong Kong standard)
s2t: Simplified Chinese to Traditional Chinese
s2tw: Simplified Chinese to Traditional Chinese (Taiwan standard)
s2twp: Simplified Chinese to Traditional Chinese (Taiwan standard, with phrases)
t2hk: Traditional Chinese to Traditional Chinese (Hong Kong standard)
t2s: Traditional Chinese to Simplified Chinese #这是我们本次项目需要用的。
t2tw: Traditional Chinese to Traditional Chinese (Taiwan standard)
tw2s: Traditional Chinese (Taiwan standard) to Simplified Chinese
tw2sp: Traditional Chinese (Taiwan standard) to Simplified Chinese (with phrases)
```

安装好后具体的使用方法，可以[参考链接](https://pypi.org/project/opencc-python-reimplemented/)
但是转换比较大的文件的时候会报 MemoryError 错误。暂时先不管这个问题，先根据那个小文件来做一下。
##### 4 字符集转换
由于后续的分词需要使用utf-8格式的字符，而上述简体字中可能存在非utf-8的字符集，避免在分词时候进行到一半而出现错误，因此先进行字符格式转换。
使用iconv命令将所有字符集转换为utf8格式
``` linux
iconv -c -t UTF-8 < wiki.zh.text.jian> wiki.zh.text.jian.utf-8 #-c 表示当转换出错的时候，放弃转换，但是程序仍然进行。 -t 表示 需要转换成的字符集。
```
ubuntu 上面 安装iconv的方法可以参考 (https://geeksww.com/tutorials/libraries/libiconv/installation/installing_libiconv_on_ubuntu_linux.php)

##### 5 分词
``` python
#4 下面就是提取文本进行分词操作，注意因为后面用gensim训练的时候，需要字符之间有空格，所以处理的时候都要加上空格。
## create dictionarly from file
def get_stopwords():
    #加载停用词表
    stopword_set = set()
    with open("./stopwords/中文停用词表.txt",'r',encoding="utf-8") as stopwords:
        for stopword in stopwords:
            stopword_set.add(stopword.strip("\n"))
    return stopword_set

'''
使用正则表达式解析文本
'''
import jieba
def parse_zhwiki(read_file_path,save_file_path):
    #过滤掉<doc> 和</doc>
    regex_str = r"(^<doc.*>$|^</doc>$)"
    #过滤掉（空格和）标点符号等
    regex_str1=r"[（ ）()\-\|!'&?$【】\"]"
    #只读取中文和英文字符。
    #regex_str2=r"[]
    docmatch=re.compile(regex_str)
    with open(read_file_path,"r",encoding="utf-8") as file:
        #写文件
        with open(save_file_path,"w+",encoding="utf-8") as output:
            content_line = file.readline()
            #获取停用词表
            stopwords = get_stopwords()
      
            while content_line:
                match_obj = docmatch.match(content_line)
                content_line = content_line.strip("\n")
                if len(content_line) > 0:
                    if not match_obj:
                    #定义一个字符串变量，表示一篇文章的分词结果
                        article_contents = ""
                        #替换
                        content_line=re.sub(regex_str1,"",content_line)
                        #使用jieba进行分词
                        words = jieba.cut(content_line,cut_all=False)
                        for word in words:
                            if word not in stopwords:
                                article_contents += word+" "#gensim 处理 需要字符之间有空格。
                        output.write(article_contents+'\n')
                content_line = file.readline()
 
#批量分词
import os
def iter_documents(top_directory):
    """
    Generator: iterate over all relevant documents, yielding one
    document (=list of utf8 tokens) at a time.
    """
    # find all .txt documents, no matter how deep under top_directory
    for root, dirs, files in os.walk(top_directory):
        #for fname in filter(lambda fname: fname.endswith('.txt'), files):
            # read each document as one big string
            for file in files:
                file_input_path=os.path.join(top_directory,file)
                file_output_path=os.path.join(top_directory,file+"parse")
                parse_zhwiki(file_input_path,file_output_path)
                print("finished:",file_input_path)
            #document = open(os.path.join(root, fname)).read()
            # break document into utf8 tokens
            #yield gensim.utils.tokenize(document, lower=True, errors='ignore')
    iter_documents("F:/ai/项目/wikiextractor-master/wikiextractor-master/extracted/AA") #调用分词。
  ```
