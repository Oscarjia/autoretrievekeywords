#### 维基百科中文预料处理步骤,参考博文(https://blog.csdn.net/qq_32166627/article/details/68942216)
版权声明：本文为CSDN博主「name_s_Jimmy」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_32166627/article/details/68942216

##### 1 下载原始数据

数据下载地址：https://dumps.wikimedia.org/zhwiki/latest/zhwiki-XXX-pages-articles.xml.bz2
下载的文件是一个大小为1.3G的压缩包，解压后是个5.8G左右的xml文件，内容是网页标签形式的。我们需要抽取出其中的有效信息。

##### 2，使用Wikipedia Extractor抽取正文

Wikipedia Extractor是意大利人用Python写的一个维基百科抽取器，使用非常方便。下载之后直接使用这条命令即可完成抽取，运行时间很快。执行以下命令。
``` python
$ sudo apt-get install unzip python python-dev python-pip
$ git clone https://github.com/attardi/wikiextractor.git wikiextractor
$ cd wikiextractor
$ sudo python setup.py install
$ ./WikiExtractor.py -b 1024M -o extracted zhwiki-latest-pages-articles.xml.bz2
```
参数-b 1024M表示以1024M为单位切分文件，默认是1M。由于最后生成的正文文本约1060M，把参数设置的大一些可以保证最后的抽取结果全部存在一个文件里。这里我们设为1024M，可以分成一个1G的大文件和一个36M的小文件，后续的步骤可以先在小文件上实验，再应用到大文件上。

这里，我们得到了2个文本文件：wiki_00, wiki_01。大小分别为：1024M, 36.7M。
